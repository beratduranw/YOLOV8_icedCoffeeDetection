{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-06T10:16:47.113979200Z",
     "start_time": "2023-06-06T10:16:47.113458500Z"
    }
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T10:16:47.322819100Z",
     "start_time": "2023-06-06T10:16:47.309909Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#model = YOLO(\"yolov8l.pt\")\n",
    "model = YOLO(\"runs/detect/train2/weights/last.pt\")\n",
    "\n",
    "model.train(data=\"config.yaml\", epochs=50, batch = 16, imgsz=480, patience=0)\n",
    "model.val()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.105  Python-3.11.3 torch-2.0.1+cu117 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "Model summary (fused): 268 layers, 43607379 parameters, 0 gradients\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning C:\\Users\\dell\\Desktop\\YOLOV8\\data\\val\\labels.cache... 17 images, 0 backgrounds, 0 corrupt: 100%|██████████| 17/17 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:03<00:00,  1.66s/it]\n",
      "                   all         17         20      0.995          1      0.995      0.866\n",
      "Speed: 0.0ms preprocess, 24.1ms inference, 0.0ms loss, 0.9ms postprocess per image\n",
      "Results saved to \u001B[1mruns\\detect\\val\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "ultralytics.yolo.utils.metrics.DetMetrics object with attributes:\n\nap_class_index: array([0])\nbox: ultralytics.yolo.utils.metrics.Metric object\nconfusion_matrix: <ultralytics.yolo.utils.metrics.ConfusionMatrix object at 0x000002B317B0DA50>\nfitness: 0.8788743421052632\nkeys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\nmaps: array([    0.86597])\nnames: {0: 'iced_coffee'}\nplot: True\nresults_dict: {'metrics/precision(B)': 0.9950253406513428, 'metrics/recall(B)': 1.0, 'metrics/mAP50(B)': 0.995, 'metrics/mAP50-95(B)': 0.8659714912280702, 'fitness': 0.8788743421052632}\nsave_dir: WindowsPath('runs/detect/val')\nspeed: {'preprocess': 0.0, 'inference': 24.14042809430291, 'loss': 0.0, 'postprocess': 0.9200292475083295}"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best = YOLO(\"runs/detect/train2/weights/best.pt\")\n",
    "best.val()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T10:16:56.239554200Z",
     "start_time": "2023-06-06T10:16:50.927912700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Model on img\n",
    "img = cv2.imread(\"data/coffee.png\")\n",
    "\n",
    "results = best.predict(img, stream=True)                       # run prediction on img\n",
    "for result in results:                                         # iterate results\n",
    "    boxes = result.boxes.cpu().numpy()                         # get boxes on cpu in numpy\n",
    "    for box in boxes:                                          # iterate boxes\n",
    "        r = box.xyxy[0].astype(int)                            # get corner points as int\n",
    "        print(r)                                               # print boxes\n",
    "        cv2.rectangle(img, r[:2], r[2:], (0, 255, 0), 2)       # draw boxes on img\n",
    "        cls = result.names[int(box.cls[0])]\n",
    "        cv2.putText(img, cls,(r[0] - 10, r[1] - 10), 0, 0.7, (0,255,0), 2)\n",
    "\n",
    "cv2.imshow(\"Image\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install labelImg\n",
    "!labelImg"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Model on Video\n",
    "green = (0, 255, 0)\n",
    "\n",
    "# Load the YOLO model\n",
    "model = YOLO(\"runs/detect/train2/weights/best.pt\")\n",
    "\n",
    "# Define the class names\n",
    "class_names = ['iced coffee']\n",
    "\n",
    "# Set the path to the input video\n",
    "video_path = \"data/Recipe.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get the frame dimensions and FPS of the input video\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Set the output path for the annotated video\n",
    "output_path = \"data/output_video_2.mp4\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "output_video = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "while True:\n",
    "    # Read the next frame from the input video\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Perform object detection on the frame using the YOLO model\n",
    "    results = model.predict(frame, conf=0.25, iou=0.1)\n",
    "\n",
    "    # Iterate over the detected objects in the frame\n",
    "    for result in results:\n",
    "        boxes = result.boxes.cpu().numpy()\n",
    "\n",
    "        if len(boxes) > 0:\n",
    "            xSum = 0\n",
    "            ySum = 0\n",
    "            for box in boxes:\n",
    "                # Get the bounding box coordinates and confidence of the detected object\n",
    "                r = box.xyxy[0].astype(int)\n",
    "                conf = box.conf[0]\n",
    "\n",
    "                # Calculate the center coordinates of the bounding box\n",
    "                center_x = int((r[0] + r[2]) / 2)\n",
    "                center_y = int((r[1] + r[3]) / 2)\n",
    "\n",
    "                xSum += center_x\n",
    "                ySum += center_y\n",
    "\n",
    "                # Draw the bounding box and center point on the frame\n",
    "                cv2.rectangle(frame, r[:2], r[2:], green, 2)\n",
    "                cv2.circle(frame, (center_x, center_y), 5, green, -1)\n",
    "\n",
    "                # Get the class label of the detected object\n",
    "                cls = result.names[int(box.cls[0])]\n",
    "                label = f\"{cls}_{conf:.2f}\"\n",
    "\n",
    "                # Draw the class label on the frame\n",
    "                cv2.putText(frame, label, (r[0] - 10, r[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "            avg_center_x = int(xSum / len(boxes))\n",
    "            avg_center_y = int(ySum / len(boxes))\n",
    "            cv2.circle(frame, (avg_center_x, avg_center_y), 5 , (255,0,0), -1)\n",
    "\n",
    "            # Add the middle and mean circles\n",
    "            cv2.circle(frame, (frame_width//2,frame_height//2), 5,(255,255,255),-1)\n",
    "            cv2.line(frame, (avg_center_x, avg_center_y), (frame_width // 2, frame_height // 2), (0, 0, 255), 2)\n",
    "\n",
    "            # Calculate the x and y differences\n",
    "            # -x solda demek +y aşağıda demek\n",
    "            diff_x = avg_center_x - frame_width // 2\n",
    "            diff_y = avg_center_y - frame_height // 2\n",
    "            cv2.putText(frame, f\"{diff_x} {diff_y}\", (20,20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "    # Write the annotated frame to the output video\n",
    "    output_video.write(frame)\n",
    "\n",
    "    # Display the annotated frame\n",
    "    cv2.imshow('Webcam', frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and output video objects\n",
    "cap.release()\n",
    "output_video.release()\n",
    "\n",
    "# Close all OpenCV windows\n",
    "cv2.destroyAllWindows()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Release the video capture and output video objects\n",
    "cap.release()\n",
    "output_video.release()\n",
    "\n",
    "# Close all OpenCV windows\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# overlap olma durumuna bak"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
